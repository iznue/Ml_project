{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# 분류 결과를 위한 상수\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# 이미지를 보여주기 위한 헬퍼(helper) 함수\n",
    "# (아래 `plot_classes_preds` 함수에서 사용)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor board 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 기본 `log_dir` 은 \"runs\"이며, 여기서는 더 구체적으로 지정하였습니다\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 임의의 학습 이미지를 가져옵니다\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 이미지 그리드를 만듭니다.\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# 이미지를 보여줍니다.\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# tensorboard에 기록합니다.\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18368), started 0:03:19 ago. (Use '!kill 18368' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-19477536db0de02b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-19477536db0de02b\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir '.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼(helper) 함수\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    데이터셋에서 n개의 임의의 데이터포인트(datapoint)와 그에 해당하는 라벨을 선택합니다\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# 임의의 이미지들과 정답(target) 인덱스를 선택합니다\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# 각 이미지의 분류 라벨(class label)을 가져옵니다\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# 임베딩(embedding) 내역을 기록합니다\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼 함수\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    학습된 신경망과 이미지 목록으로부터 예측 결과 및 확률을 생성합니다\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    학습된 신경망과 배치로부터 가져온 이미지 / 라벨을 사용하여 matplotlib\n",
    "    Figure를 생성합니다. 이는 신경망의 예측 결과 / 확률과 함께 정답을 보여주며,\n",
    "    예측 결과가 맞았는지 여부에 따라 색을 다르게 표시합니다. \"images_to_probs\"\n",
    "    함수를 사용합니다.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # 배치에서 이미지를 가져와 예측 결과 / 정답과 함께 표시(plot)합니다\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m \u001b[39m# 순전파 + 역전파 + 최적화를 한 후\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[0;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[0;32m     13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # 데이터셋을 여러번 반복\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n",
    "        inputs, labels = data\n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # 매 1000 미니배치마다...\n",
    "\n",
    "            # ...학습 중 손실(running loss)을 기록하고\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...무작위 미니배치(mini-batch)에 대한 모델의 예측 결과를 보여주도록\n",
    "            # Matplotlib Figure를 기록합니다\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m class_label \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m testloader:\n\u001b[0;32m      8\u001b[0m         images, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m      9\u001b[0m         output \u001b[39m=\u001b[39m net(images)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\multiprocessing\\queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    121\u001b[0m \u001b[39m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39;49mloads(res)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:332\u001b[0m, in \u001b[0;36mrebuild_storage_filename\u001b[1;34m(cls, manager, handle, size, dtype)\u001b[0m\n\u001b[0;32m    327\u001b[0m     untyped_storage: torch\u001b[39m.\u001b[39mUntypedStorage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mUntypedStorage\u001b[39m.\u001b[39m_new_shared_filename_cpu(manager, handle, byte_size)\n\u001b[0;32m    328\u001b[0m     storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m    329\u001b[0m         wrap_storage\u001b[39m=\u001b[39muntyped_storage,\n\u001b[0;32m    330\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    331\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 332\u001b[0m shared_cache[handle] \u001b[39m=\u001b[39m StorageWeakRef(storage)\n\u001b[0;32m    333\u001b[0m \u001b[39mreturn\u001b[39;00m storage\u001b[39m.\u001b[39m_shared_decref()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\gpu\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:32\u001b[0m, in \u001b[0;36mStorageWeakRef.__init__\u001b[1;34m(self, storage)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcdata \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39m_weak_ref()\n\u001b[0;32m     30\u001b[0m \u001b[39m# Save a direct reference to _free_weak_ref because the `torch` module\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# might be cleared during Python shutdown before this module is cleared.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_free_weak_ref \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mStorage\u001b[39m.\u001b[39m_free_weak_ref\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. 예측 확률을 test_size x num_classes 텐서로 가져옵니다\n",
    "# 2. 예측 결과를 test_size 텐서로 가져옵니다\n",
    "# 실행하는데 10초 이하 소요\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# 헬퍼 함수\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    0부터 9까지의 \"class_index\"를 가져온 후 해당 정밀도-재현율(precision-recall)\n",
    "    곡선을 그립니다\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# 모든 정밀도-재현율(precision-recall; pr) 곡선을 그립니다\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
