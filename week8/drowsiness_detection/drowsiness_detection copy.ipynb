{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe & OpenCV project : Drowsiness Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe를 이용해 양쪽 눈에 대한 landmark(index) 포인트를 가져옴\n",
    "\n",
    "mp_facemesh = mp.solutions.face_mesh\n",
    "mp_drawing  = mp.solutions.drawing_utils\n",
    "denormalize_coordinates = mp_drawing._normalized_to_pixel_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceMash 설정\n",
    "\n",
    "def get_facemesh(\n",
    "                max_num_faces=1,                # 감지할 얼굴 수\n",
    "                refine_landmarks=False,         # 눈 외의 landmark는 세분화시키지 않음\n",
    "                min_detection_confidence=0.5,   # 얼굴 인식에 성공한 것으로 간주되는 최소 신뢰도\n",
    "                min_tracking_confidence= 0.5    # 성공적으로 추적한 것으로 간주되는 최소 신뢰도\n",
    "):\n",
    "    face_mesh = mp_facemesh.FaceMesh(\n",
    "        max_num_faces=max_num_faces,\n",
    "        refine_landmarks=refine_landmarks,\n",
    "        min_detection_confidence=min_detection_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence\n",
    "    )\n",
    "\n",
    "    return face_mesh\n",
    "# 감지된 landmark points 목록\n",
    "# face_mesh.multi_face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point_1, point_2):\n",
    "    # L2 norm 계산 (두 벡터 사이의 거리 계산)\n",
    "    dist = sum([(i - j) ** 2 for i, j in zip(point_1, point_2)]) ** 0.5\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR 공식 적용\n",
    "\n",
    "def get_ear(landmarks, refer_idxs, frame_width, frame_height):\n",
    "    # landmarks : 검출된 lanmarks list\n",
    "    # refer_idxs : 검출을 위해 지정한 landmarks list [index]\n",
    "\n",
    "    try:\n",
    "        # 수평 거리 계산\n",
    "        coords_points = []\n",
    "        for i in refer_idxs:\n",
    "            lm = landmarks[i]\n",
    "            coord = denormalize_coordinates(lm.x, lm.y, frame_width, frame_height)\n",
    "            coords_points.append(coord)\n",
    " \n",
    "        # EAR 공식에 맞춰 P2-P6, P3-P5, P1-P4를 연산함\n",
    "        P2_P6 = distance(coords_points[1], coords_points[5])\n",
    "        P3_P5 = distance(coords_points[2], coords_points[4])\n",
    "        P1_P4 = distance(coords_points[0], coords_points[3])\n",
    " \n",
    "        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)\n",
    " \n",
    "    except:\n",
    "        ear = 0.0\n",
    "        coords_points = None\n",
    " \n",
    "    return ear, coords_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_ear(landmarks, left_eye_idxs, right_eye_idxs, image_w, image_h):\n",
    "    \n",
    "    # 왼쪽 눈의 EAR 값과 landmarks의 좌표 값을 반환함\n",
    "    left_ear, left_lm_coordinates = get_ear(\n",
    "                                      landmarks, \n",
    "                                      left_eye_idxs, \n",
    "                                      image_w, \n",
    "                                      image_h\n",
    "                                    )\n",
    "    \n",
    "    # 오른쪽 눈의 EAR 값과 landmarks의 좌표 값을 반환함\n",
    "    right_ear, right_lm_coordinates = get_ear(\n",
    "                                      landmarks, \n",
    "                                      right_eye_idxs, \n",
    "                                      image_w, \n",
    "                                      image_h\n",
    "                                    )\n",
    "    # 최종 EAR 값을 얻기 위해 왼쪽 오른쪽의 EAR 값 평균을 계산함\n",
    "    Avg_EAR = (left_ear + right_ear) / 2.0\n",
    " \n",
    "    return Avg_EAR, (left_lm_coordinates, right_lm_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eye_landmarks(frame, left_lm_coordinates, \n",
    "                       right_lm_coordinates, color\n",
    "                       ):\n",
    "    for lm_coordinates in [left_lm_coordinates, right_lm_coordinates]:\n",
    "        if lm_coordinates:\n",
    "            for coord in lm_coordinates:\n",
    "                cv2.circle(frame, coord, 2, color, -1)\n",
    " \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text(image, text, origin, \n",
    "              color, font=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "              fntScale=0.8, thickness=2\n",
    "              ):\n",
    "    image = cv2.putText(image, text, origin, font, fntScale, color, thickness)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video:\n",
    "    def __init__(self):\n",
    "        self.red = (0,0,255)\n",
    "        self.green = (0,255,0)\n",
    "        self.white = (255,255,255)\n",
    "\n",
    "        self.eye_idxs = {\n",
    "            \"left\": [362, 385, 387, 263, 373, 380],\n",
    "            \"right\": [33, 160, 158, 133, 153, 144],\n",
    "        }\n",
    "\n",
    "        self.facemesh_model = get_facemesh()\n",
    "\n",
    "        self.state_tracker = {\n",
    "            \"start_time\": time.perf_counter(),\n",
    "            \"DROWSY_TIME\": 0.0,  # Holds time passed with EAR < EAR_THRESH\n",
    "            \"COLOR\": self.green,\n",
    "            \"play_alarm\": False,\n",
    "        }\n",
    "        \n",
    "        self.EAR_txt_pos = (10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(self, frame: np.array, thresholds : dict):\n",
    "\n",
    "        frame.flags.writeable = False\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "        DROWSY_TIME_txt_pos = (10, int(frame_h // 2 * 1.7))\n",
    "        ALM_txt_pos = (10, int(frame_h // 2 * 1.85))\n",
    " \n",
    "        results = self.facemesh_model.process(frame)\n",
    " \n",
    "        if results.multi_face_landmarks:\n",
    "            landmarks = results.multi_face_landmarks[0].landmark\n",
    "            EAR, coordinates = calculate_avg_ear(landmarks,\n",
    "                                                 self.eye_idxs[\"left\"], \n",
    "                                                 self.eye_idxs[\"right\"], \n",
    "                                                 frame_w, \n",
    "                                                 frame_h\n",
    "                                                 )\n",
    "            frame = plot_eye_landmarks(frame, \n",
    "                                       coordinates[0], \n",
    "                                       coordinates[1],\n",
    "                                       self.state_tracker[\"COLOR\"]\n",
    "                                       )\n",
    " \n",
    "            if EAR < thresholds[\"EAR_THRESH\"]:\n",
    " \n",
    "                end_time = time.perf_counter()\n",
    " \n",
    "                self.state_tracker[\"DROWSY_TIME\"] += end_time - self.state_tracker[\"start_time\"]\n",
    "                self.state_tracker[\"start_time\"] = end_time\n",
    "                self.state_tracker[\"COLOR\"] = self.red\n",
    " \n",
    "                if self.state_tracker[\"DROWSY_TIME\"] >= thresholds[\"WAIT_TIME\"]:\n",
    "                    self.state_tracker[\"play_alarm\"] = True\n",
    "                    plot_text(frame, \"WAKE UP! WAKE UP\", \n",
    "                              ALM_txt_pos, self.state_tracker[\"COLOR\"])\n",
    " \n",
    "            else:\n",
    "                self.state_tracker[\"start_time\"] = time.perf_counter()\n",
    "                self.state_tracker[\"DROWSY_TIME\"] = 0.0\n",
    "                self.state_tracker[\"COLOR\"] = self.green\n",
    "                self.state_tracker[\"play_alarm\"] = False\n",
    " \n",
    "            EAR_txt = f\"EAR: {round(EAR, 2)}\"\n",
    "            DROWSY_TIME_txt = f\"DROWSY: {round(self.state_tracker['DROWSY_TIME'], 3)} Secs\"\n",
    "            plot_text(frame, EAR_txt, \n",
    "                      self.EAR_txt_pos, self.state_tracker[\"COLOR\"])\n",
    "            plot_text(frame, DROWSY_TIME_txt, \n",
    "                      DROWSY_TIME_txt_pos, self.state_tracker[\"COLOR\"])\n",
    " \n",
    "        else:\n",
    "            self.state_tracker[\"start_time\"] = time.perf_counter()\n",
    "            self.state_tracker[\"DROWSY_TIME\"] = 0.0\n",
    "            self.state_tracker[\"COLOR\"] = self.green\n",
    "            self.state_tracker[\"play_alarm\"] = False\n",
    " \n",
    "            frame = cv2.flip(frame, 1)\n",
    " \n",
    "        return frame, self.state_tracker[\"play_alarm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Video' object has no attribute 'process'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m     19\u001b[0m     video_handler \u001b[39m=\u001b[39m Video()\n\u001b[1;32m---> 20\u001b[0m     frame, play_alarm \u001b[39m=\u001b[39m video_handler\u001b[39m.\u001b[39;49mprocess(frame, thresholds)\n\u001b[0;32m     23\u001b[0m webcam\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m     24\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Video' object has no attribute 'process'"
     ]
    }
   ],
   "source": [
    "webcam = cv2.VideoCapture(0)\n",
    "webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "fps=webcam.get(4)\n",
    "\n",
    "thresholds = {\n",
    "    \"EAR_THRESH\": 0.4,\n",
    "    \"WAIT_TIME\": 5.0,\n",
    "}\n",
    "\n",
    "while webcam.isOpened():\n",
    "    status, frame = webcam.read()\n",
    "\n",
    "    if not status:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    video_handler = Video()\n",
    "    frame, play_alarm = video_handler.process(frame, thresholds)\n",
    "\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
