{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe & OpenCV & Pytorch project : Drowsiness Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [var for var in globals() if var[0] != \"_\"]\n",
    "for var in all:\n",
    "    del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe를 이용해 양쪽 눈에 대한 landmark(index) 포인트를 가져옴\n",
    "\n",
    "mp_facemesh = mp.solutions.face_mesh\n",
    "mp_drawing  = mp.solutions.drawing_utils\n",
    "denormalize_coordinates = mp_drawing._normalized_to_pixel_coordinates\n",
    "\n",
    "Class_Names = ['Closed_Eyes', 'Open_Eyes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = torch.load('drowsiness_detect.pt').to(device)\n",
    "model.load_state_dict(torch.load('drowsiness_detect_state_dict.pt')) \n",
    "\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceMash 설정\n",
    "\n",
    "def get_facemesh(\n",
    "                max_num_faces=1,                # 감지할 얼굴 수\n",
    "                refine_landmarks=False,         # 눈 외의 landmark는 세분화시키지 않음\n",
    "                min_detection_confidence=0.1,   # 얼굴 인식에 성공한 것으로 간주되는 최소 신뢰도\n",
    "                min_tracking_confidence= 0.2    # 성공적으로 추적한 것으로 간주되는 최소 신뢰도\n",
    "):\n",
    "    face_mesh = mp_facemesh.FaceMesh(\n",
    "        max_num_faces=max_num_faces,\n",
    "        refine_landmarks=refine_landmarks,\n",
    "        min_detection_confidence=min_detection_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence\n",
    "    )\n",
    "\n",
    "    return face_mesh\n",
    "# 감지된 landmark points 목록\n",
    "# face_mesh.multi_face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point_1, point_2):\n",
    "    # L2 norm 계산 (두 벡터 사이의 거리 계산)\n",
    "    dist = sum([(i - j) ** 2 for i, j in zip(point_1, point_2)]) ** 0.5\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR 공식 적용\n",
    "\n",
    "def get_ear(landmarks, refer_idxs, frame_width, frame_height):\n",
    "    # landmarks : 검출된 lanmarks list\n",
    "    # refer_idxs : 검출을 위해 지정한 landmarks list [index]\n",
    "\n",
    "    try:\n",
    "        # 수평 거리 계산\n",
    "        coords_points = []\n",
    "        for i in refer_idxs:\n",
    "            lm = landmarks[i]\n",
    "            coord = denormalize_coordinates(lm.x, lm.y, frame_width, frame_height)\n",
    "            coords_points.append(coord)\n",
    " \n",
    "        # EAR 공식에 맞춰 P2-P6, P3-P5, P1-P4를 연산함\n",
    "        P2_P6 = distance(coords_points[1], coords_points[5])\n",
    "        P3_P5 = distance(coords_points[2], coords_points[4])\n",
    "        P1_P4 = distance(coords_points[0], coords_points[3])\n",
    " \n",
    "        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)\n",
    " \n",
    "    except:\n",
    "        ear = 0.0\n",
    "        coords_points = None\n",
    " \n",
    "    return ear, coords_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_ear(landmarks, left_eye_idxs, right_eye_idxs, image_w, image_h):\n",
    "    \n",
    "    # 왼쪽 눈의 EAR 값과 landmarks의 좌표 값을 반환함\n",
    "    left_ear, left_lm_coordinates = get_ear(\n",
    "                                      landmarks, \n",
    "                                      left_eye_idxs, \n",
    "                                      image_w, \n",
    "                                      image_h\n",
    "                                    )\n",
    "    \n",
    "    # 오른쪽 눈의 EAR 값과 landmarks의 좌표 값을 반환함\n",
    "    right_ear, right_lm_coordinates = get_ear(\n",
    "                                      landmarks, \n",
    "                                      right_eye_idxs, \n",
    "                                      image_w, \n",
    "                                      image_h\n",
    "                                    )\n",
    "    # 최종 EAR 값을 얻기 위해 왼쪽 오른쪽의 EAR 값 평균을 계산함\n",
    "    Avg_EAR = (left_ear + right_ear) / 2.0\n",
    " \n",
    "    return Avg_EAR, (left_lm_coordinates, right_lm_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eye_landmarks(frame, left_lm_coordinates, \n",
    "                       right_lm_coordinates, color\n",
    "                       ):\n",
    "    for lm_coordinates in [left_lm_coordinates, right_lm_coordinates]:\n",
    "        if lm_coordinates:\n",
    "            for coord in lm_coordinates:\n",
    "                cv2.circle(frame, coord, 2, color, -1)\n",
    " \n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text(image, text, origin, \n",
    "              color, font=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "              fntScale=0.8, thickness=2\n",
    "              ):\n",
    "    image = cv2.putText(image, text, origin, font, fntScale, color, thickness)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eyeslabel(crop_eye):\n",
    "    # eye = cv2.cvtColor(crop_eye, cv2.COLOR_BGR2RGB)\n",
    "    # crop_eye = cv2.cvtColor(crop_eye, cv2.COLOR_RGB2GRAY)\n",
    "    eye_img = Image.fromarray(crop_eye)\n",
    "\n",
    "    eye_img = transform(eye_img).to(device)\n",
    "    eye_img = eye_img.unsqueeze(0)\n",
    "    \n",
    "    test_preds = model(eye_img)\n",
    "    predicted_class_idx = torch.argmax(test_preds, dim=1)\n",
    "    predicted_class_label = Class_Names[predicted_class_idx.item()]\n",
    "\n",
    "    return predicted_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alarm(path):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(path)\n",
    "    pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(frame: np.array, thresholds : dict):\n",
    "\n",
    "        frame.flags.writeable = False\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "        \n",
    "        DROWSY_TIME_txt_pos = (10, int(frame_h // 2 * 1.7))\n",
    "        ALM_txt_pos = (10, int(frame_h // 2 * 1.85))\n",
    "\n",
    "        red = (0,0,255)\n",
    "        green = (0,255,0)\n",
    " \n",
    "        facemesh_model = get_facemesh()\n",
    "        results = facemesh_model.process(frame)\n",
    "\n",
    "        chosen_left_eye_idxs  = [362, 385, 387, 263, 373, 380]\n",
    "        chosen_right_eye_idxs = [33,  160, 158, 133, 153, 144]\n",
    " \n",
    "        state_tracker = {\n",
    "            \"start_time\": time.perf_counter(),\n",
    "            \"DROWSY_TIME\": 0.0,  # Holds time passed with EAR < EAR_THRESH\n",
    "            \"COLOR\": green,\n",
    "            \"play_alarm\": False,\n",
    "        }\n",
    "\n",
    "        if results.multi_face_landmarks:        \n",
    "\n",
    "            landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "            ############################################################### EAR 계산 및 감지한 eye landmark 시각화\n",
    "            EAR, coordinates = calculate_avg_ear(landmarks,\n",
    "                                                 chosen_left_eye_idxs, \n",
    "                                                 chosen_right_eye_idxs, \n",
    "                                                 frame_w, \n",
    "                                                 frame_h\n",
    "                                                 )\n",
    "            # frame = plot_eye_landmarks(frame, \n",
    "            #                            coordinates[0], \n",
    "            #                            coordinates[1],\n",
    "            #                            state_tracker[\"COLOR\"]\n",
    "            #                            )\n",
    "            \n",
    "            ############################################################### model에 적용할 eye 영역 추출 및 결과 출력\n",
    "            global eye, eye_left, eye_right\n",
    "            # eye = frame[coordinates[1][2][1]-10 : coordinates[0][4][1]+10, coordinates[1][0][0]-10 : coordinates[0][3][0]+10]\n",
    "            eye_left = frame[coordinates[0][2][1]-10 : coordinates[0][4][1]+10, coordinates[0][0][0]-10 : coordinates[0][3][0]+10]\n",
    "            eye_right = frame[coordinates[1][2][1]-10 : coordinates[1][4][1]+10, coordinates[1][0][0]-10 : coordinates[1][3][0]+10]\n",
    "\n",
    "            global model_result, left_pred, right_pred\n",
    "            # model_result = pred_eyes(eye)\n",
    "            left_pred = pred_eyeslabel(eye_left)\n",
    "            right_pred = pred_eyeslabel(eye_right)\n",
    "\n",
    "\n",
    "            ############################################################### EAR 연산값 출력 및 알람 기능\n",
    "            if EAR < thresholds[\"EAR_THRESH\"] and right_pred=='Closed_Eyes' and left_pred=='Closed_Eyes':\n",
    " \n",
    "                end_time = time.perf_counter()\n",
    "                # end_time = time.process_time()\n",
    " \n",
    "                state_tracker[\"DROWSY_TIME\"] += end_time - state_tracker[\"start_time\"]\n",
    "                state_tracker[\"start_time\"] = end_time\n",
    "                state_tracker[\"COLOR\"] = red\n",
    " \n",
    "                if state_tracker[\"DROWSY_TIME\"] >= thresholds[\"WAIT_TIME\"]:\n",
    "                    state_tracker[\"play_alarm\"] = True\n",
    "                    plot_text(frame, \"WAKE UP! WAKE UP\", \n",
    "                              ALM_txt_pos, state_tracker[\"COLOR\"])\n",
    "                    alarm('alarm.wav')\n",
    " \n",
    "            else:\n",
    "                state_tracker[\"start_time\"] = time.perf_counter() #time.process_time()\n",
    "                state_tracker[\"DROWSY_TIME\"] = 0.0\n",
    "                state_tracker[\"COLOR\"] = green\n",
    "                state_tracker[\"play_alarm\"] = False\n",
    " \n",
    "            EAR_txt = f\"EAR: {round(EAR, 2)}\"\n",
    "            DROWSY_TIME_txt = f\"DROWSY: {round(state_tracker['DROWSY_TIME'], 3)} Secs\"\n",
    "            plot_text(frame, EAR_txt, \n",
    "                      (10,30), state_tracker[\"COLOR\"])\n",
    "            plot_text(frame, DROWSY_TIME_txt, \n",
    "                      DROWSY_TIME_txt_pos, state_tracker[\"COLOR\"])\n",
    " \n",
    "        else:\n",
    "            state_tracker[\"start_time\"] = time.perf_counter() #time.process_time()\n",
    "            state_tracker[\"DROWSY_TIME\"] = 0.0\n",
    "            state_tracker[\"COLOR\"] = green\n",
    "            state_tracker[\"play_alarm\"] = False\n",
    " \n",
    "            # frame = cv2.flip(frame, 1)\n",
    " \n",
    "        return frame, state_tracker[\"play_alarm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Open_Eyes Open_Eyes\n",
      "1 Open_Eyes Open_Eyes\n",
      "2 Open_Eyes Open_Eyes\n",
      "3 Open_Eyes Open_Eyes\n",
      "4 Open_Eyes Open_Eyes\n",
      "5 Open_Eyes Open_Eyes\n",
      "6 Open_Eyes Open_Eyes\n",
      "7 Open_Eyes Open_Eyes\n",
      "8 Open_Eyes Open_Eyes\n",
      "9 Open_Eyes Open_Eyes\n",
      "10 Open_Eyes Open_Eyes\n",
      "11 Open_Eyes Open_Eyes\n",
      "12 Open_Eyes Open_Eyes\n",
      "13 Open_Eyes Open_Eyes\n",
      "14 Open_Eyes Open_Eyes\n",
      "15 Open_Eyes Open_Eyes\n",
      "16 Open_Eyes Open_Eyes\n",
      "17 Open_Eyes Open_Eyes\n",
      "18 Open_Eyes Open_Eyes\n",
      "19 Open_Eyes Open_Eyes\n",
      "20 Open_Eyes Closed_Eyes\n",
      "21 Open_Eyes Closed_Eyes\n",
      "22 Open_Eyes Closed_Eyes\n",
      "23 Open_Eyes Closed_Eyes\n",
      "24 Open_Eyes Closed_Eyes\n",
      "25 Open_Eyes Open_Eyes\n",
      "26 Open_Eyes Closed_Eyes\n",
      "27 Open_Eyes Open_Eyes\n",
      "28 Open_Eyes Open_Eyes\n",
      "29 Open_Eyes Open_Eyes\n",
      "30 Open_Eyes Open_Eyes\n",
      "31 Open_Eyes Open_Eyes\n",
      "32 Closed_Eyes Open_Eyes\n",
      "33 Open_Eyes Open_Eyes\n",
      "34 Open_Eyes Closed_Eyes\n",
      "35 Closed_Eyes Closed_Eyes\n",
      "36 Closed_Eyes Closed_Eyes\n",
      "37 Open_Eyes Open_Eyes\n",
      "38 Open_Eyes Open_Eyes\n",
      "39 Closed_Eyes Open_Eyes\n",
      "40 Closed_Eyes Closed_Eyes\n",
      "41 Closed_Eyes Closed_Eyes\n",
      "42 Closed_Eyes Open_Eyes\n",
      "43 Closed_Eyes Open_Eyes\n",
      "44 Closed_Eyes Open_Eyes\n",
      "45 Closed_Eyes Open_Eyes\n",
      "46 Closed_Eyes Closed_Eyes\n",
      "47 Closed_Eyes Closed_Eyes\n",
      "48 Closed_Eyes Closed_Eyes\n",
      "49 Closed_Eyes Closed_Eyes\n",
      "50 Closed_Eyes Closed_Eyes\n",
      "51 Closed_Eyes Closed_Eyes\n",
      "52 Closed_Eyes Closed_Eyes\n",
      "53 Closed_Eyes Open_Eyes\n",
      "54 Closed_Eyes Closed_Eyes\n",
      "55 Closed_Eyes Closed_Eyes\n",
      "56 Closed_Eyes Closed_Eyes\n",
      "57 Closed_Eyes Closed_Eyes\n",
      "58 Closed_Eyes Closed_Eyes\n",
      "59 Closed_Eyes Closed_Eyes\n",
      "60 Closed_Eyes Closed_Eyes\n",
      "61 Closed_Eyes Closed_Eyes\n",
      "62 Closed_Eyes Closed_Eyes\n",
      "63 Closed_Eyes Closed_Eyes\n",
      "64 Closed_Eyes Closed_Eyes\n",
      "65 Closed_Eyes Closed_Eyes\n",
      "66 Closed_Eyes Closed_Eyes\n",
      "67 Closed_Eyes Closed_Eyes\n",
      "68 Closed_Eyes Closed_Eyes\n",
      "69 Closed_Eyes Closed_Eyes\n",
      "70 Closed_Eyes Closed_Eyes\n",
      "71 Closed_Eyes Closed_Eyes\n",
      "72 Closed_Eyes Closed_Eyes\n",
      "73 Closed_Eyes Closed_Eyes\n",
      "74 Closed_Eyes Closed_Eyes\n",
      "75 Closed_Eyes Closed_Eyes\n",
      "76 Closed_Eyes Closed_Eyes\n",
      "77 Closed_Eyes Closed_Eyes\n",
      "78 Closed_Eyes Closed_Eyes\n",
      "79 Closed_Eyes Closed_Eyes\n",
      "80 Closed_Eyes Open_Eyes\n",
      "81 Closed_Eyes Closed_Eyes\n",
      "82 Closed_Eyes Closed_Eyes\n",
      "83 Closed_Eyes Closed_Eyes\n",
      "84 Closed_Eyes Closed_Eyes\n",
      "85 Closed_Eyes Closed_Eyes\n",
      "86 Closed_Eyes Open_Eyes\n",
      "87 Closed_Eyes Open_Eyes\n",
      "88 Closed_Eyes Open_Eyes\n",
      "89 Closed_Eyes Closed_Eyes\n",
      "90 Closed_Eyes Closed_Eyes\n",
      "91 Closed_Eyes Closed_Eyes\n",
      "92 Closed_Eyes Closed_Eyes\n",
      "93 Closed_Eyes Closed_Eyes\n",
      "94 Closed_Eyes Closed_Eyes\n",
      "95 Open_Eyes Open_Eyes\n",
      "96 Open_Eyes Open_Eyes\n",
      "97 Closed_Eyes Open_Eyes\n",
      "98 Closed_Eyes Open_Eyes\n",
      "99 Closed_Eyes Open_Eyes\n",
      "100 Open_Eyes Open_Eyes\n",
      "101 Closed_Eyes Open_Eyes\n",
      "102 Open_Eyes Open_Eyes\n",
      "103 Open_Eyes Open_Eyes\n",
      "104 Closed_Eyes Open_Eyes\n",
      "105 Open_Eyes Closed_Eyes\n",
      "106 Closed_Eyes Closed_Eyes\n",
      "107 Open_Eyes Open_Eyes\n",
      "108 Open_Eyes Open_Eyes\n",
      "109 Open_Eyes Open_Eyes\n",
      "110 Closed_Eyes Closed_Eyes\n",
      "111 Closed_Eyes Open_Eyes\n",
      "112 Closed_Eyes Open_Eyes\n",
      "113 Open_Eyes Closed_Eyes\n",
      "114 Open_Eyes Open_Eyes\n",
      "115 Open_Eyes Open_Eyes\n",
      "116 Open_Eyes Open_Eyes\n",
      "117 Closed_Eyes Open_Eyes\n",
      "118 Open_Eyes Open_Eyes\n",
      "119 Closed_Eyes Open_Eyes\n",
      "120 Open_Eyes Open_Eyes\n",
      "121 Closed_Eyes Open_Eyes\n",
      "122 Open_Eyes Open_Eyes\n",
      "123 Closed_Eyes Closed_Eyes\n",
      "124 Closed_Eyes Closed_Eyes\n",
      "125 Closed_Eyes Closed_Eyes\n",
      "126 Closed_Eyes Open_Eyes\n",
      "127 Closed_Eyes Closed_Eyes\n",
      "128 Closed_Eyes Closed_Eyes\n",
      "129 Closed_Eyes Closed_Eyes\n",
      "130 Open_Eyes Open_Eyes\n",
      "131 Closed_Eyes Open_Eyes\n",
      "132 Closed_Eyes Closed_Eyes\n",
      "133 Closed_Eyes Closed_Eyes\n",
      "134 Closed_Eyes Closed_Eyes\n",
      "135 Closed_Eyes Closed_Eyes\n",
      "136 Closed_Eyes Closed_Eyes\n",
      "137 Closed_Eyes Open_Eyes\n",
      "138 Closed_Eyes Closed_Eyes\n",
      "139 Closed_Eyes Open_Eyes\n",
      "140 Closed_Eyes Closed_Eyes\n",
      "141 Closed_Eyes Closed_Eyes\n",
      "142 Closed_Eyes Closed_Eyes\n",
      "143 Closed_Eyes Closed_Eyes\n",
      "144 Closed_Eyes Closed_Eyes\n",
      "145 Closed_Eyes Closed_Eyes\n",
      "146 Closed_Eyes Closed_Eyes\n",
      "147 Closed_Eyes Closed_Eyes\n",
      "148 Closed_Eyes Closed_Eyes\n",
      "149 Closed_Eyes Open_Eyes\n",
      "150 Closed_Eyes Closed_Eyes\n",
      "151 Closed_Eyes Closed_Eyes\n",
      "152 Closed_Eyes Closed_Eyes\n",
      "153 Closed_Eyes Closed_Eyes\n",
      "154 Closed_Eyes Closed_Eyes\n",
      "155 Closed_Eyes Closed_Eyes\n",
      "156 Closed_Eyes Closed_Eyes\n",
      "157 Closed_Eyes Open_Eyes\n",
      "158 Closed_Eyes Closed_Eyes\n",
      "159 Closed_Eyes Closed_Eyes\n",
      "160 Open_Eyes Closed_Eyes\n",
      "161 Closed_Eyes Closed_Eyes\n",
      "162 Closed_Eyes Open_Eyes\n",
      "163 Closed_Eyes Closed_Eyes\n",
      "164 Closed_Eyes Closed_Eyes\n",
      "165 Closed_Eyes Closed_Eyes\n",
      "166 Closed_Eyes Closed_Eyes\n",
      "167 Closed_Eyes Closed_Eyes\n",
      "168 Closed_Eyes Closed_Eyes\n",
      "169 Closed_Eyes Closed_Eyes\n",
      "170 Closed_Eyes Closed_Eyes\n",
      "171 Closed_Eyes Closed_Eyes\n",
      "172 Closed_Eyes Closed_Eyes\n",
      "173 Closed_Eyes Closed_Eyes\n",
      "174 Closed_Eyes Closed_Eyes\n",
      "175 Closed_Eyes Closed_Eyes\n",
      "176 Closed_Eyes Closed_Eyes\n",
      "177 Closed_Eyes Closed_Eyes\n",
      "178 Closed_Eyes Closed_Eyes\n",
      "179 Closed_Eyes Closed_Eyes\n",
      "180 Closed_Eyes Closed_Eyes\n",
      "181 Closed_Eyes Closed_Eyes\n",
      "182 Closed_Eyes Closed_Eyes\n",
      "183 Closed_Eyes Closed_Eyes\n",
      "184 Closed_Eyes Closed_Eyes\n",
      "185 Closed_Eyes Closed_Eyes\n",
      "186 Closed_Eyes Closed_Eyes\n",
      "187 Closed_Eyes Closed_Eyes\n",
      "188 Closed_Eyes Closed_Eyes\n",
      "189 Closed_Eyes Closed_Eyes\n",
      "190 Closed_Eyes Closed_Eyes\n",
      "191 Open_Eyes Closed_Eyes\n",
      "192 Open_Eyes Open_Eyes\n",
      "193 Open_Eyes Closed_Eyes\n",
      "194 Open_Eyes Open_Eyes\n",
      "195 Open_Eyes Open_Eyes\n",
      "196 Open_Eyes Open_Eyes\n",
      "197 Open_Eyes Open_Eyes\n",
      "198 Open_Eyes Open_Eyes\n",
      "199 Open_Eyes Open_Eyes\n",
      "200 Open_Eyes Open_Eyes\n",
      "201 Open_Eyes Open_Eyes\n",
      "202 Open_Eyes Open_Eyes\n",
      "203 Open_Eyes Open_Eyes\n",
      "204 Open_Eyes Open_Eyes\n",
      "205 Open_Eyes Closed_Eyes\n",
      "206 Closed_Eyes Open_Eyes\n",
      "207 Closed_Eyes Open_Eyes\n",
      "208 Closed_Eyes Closed_Eyes\n",
      "209 Closed_Eyes Closed_Eyes\n",
      "210 Closed_Eyes Open_Eyes\n",
      "211 Closed_Eyes Open_Eyes\n",
      "212 Open_Eyes Open_Eyes\n",
      "213 Closed_Eyes Closed_Eyes\n",
      "214 Closed_Eyes Closed_Eyes\n",
      "215 Open_Eyes Open_Eyes\n",
      "216 Open_Eyes Open_Eyes\n",
      "217 Open_Eyes Open_Eyes\n",
      "218 Open_Eyes Open_Eyes\n",
      "219 Open_Eyes Open_Eyes\n",
      "220 Closed_Eyes Closed_Eyes\n",
      "221 Closed_Eyes Closed_Eyes\n",
      "222 Closed_Eyes Closed_Eyes\n",
      "223 Closed_Eyes Closed_Eyes\n",
      "224 Closed_Eyes Closed_Eyes\n",
      "225 Closed_Eyes Closed_Eyes\n",
      "226 Closed_Eyes Closed_Eyes\n",
      "227 Closed_Eyes Closed_Eyes\n",
      "228 Open_Eyes Closed_Eyes\n",
      "229 Closed_Eyes Closed_Eyes\n",
      "230 Closed_Eyes Closed_Eyes\n",
      "231 Closed_Eyes Open_Eyes\n",
      "232 Closed_Eyes Open_Eyes\n",
      "233 Closed_Eyes Open_Eyes\n",
      "234 Closed_Eyes Open_Eyes\n",
      "235 Closed_Eyes Open_Eyes\n",
      "236 Closed_Eyes Open_Eyes\n",
      "237 Closed_Eyes Open_Eyes\n",
      "238 Open_Eyes Open_Eyes\n",
      "239 Open_Eyes Closed_Eyes\n",
      "240 Open_Eyes Open_Eyes\n",
      "241 Open_Eyes Open_Eyes\n",
      "242 Closed_Eyes Closed_Eyes\n",
      "243 Closed_Eyes Closed_Eyes\n",
      "244 Open_Eyes Closed_Eyes\n",
      "245 Open_Eyes Closed_Eyes\n",
      "246 Open_Eyes Closed_Eyes\n",
      "247 Open_Eyes Closed_Eyes\n",
      "248 Closed_Eyes Open_Eyes\n",
      "249 Closed_Eyes Closed_Eyes\n",
      "250 Closed_Eyes Closed_Eyes\n",
      "251 Closed_Eyes Closed_Eyes\n",
      "252 Closed_Eyes Open_Eyes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m         cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, frame)\n\u001b[0;32m     29\u001b[0m         frame_cnt\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m30\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m): \u001b[39m# 0또는 1의 경우 계속적으로 읽어옴\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     34\u001b[0m webcam\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# webcam = cv2.VideoCapture(0)\n",
    "webcam = cv2.VideoCapture('d:/drowsiness_video_data/SGA2100300S0042.mp4')\n",
    "\n",
    "webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640) #1920, 640\n",
    "webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) #1080, 480\n",
    "\n",
    "thresholds = {\n",
    "    \"EAR_THRESH\": 0.22, #0.25 (videocapture:0)\n",
    "    \"WAIT_TIME\": 0.035, #0.02 (videocapture:0), 0.032\n",
    "}\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "frame_cnt=0\n",
    "\n",
    "while webcam.isOpened():\n",
    "    status, frame = webcam.read()\n",
    "\n",
    "    if status:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  #외부 영상 재생 시에 설정\n",
    "\n",
    "        frame, play_alarm = process(frame, thresholds)\n",
    "        print(frame_cnt, right_pred, left_pred)\n",
    "\n",
    "        # cv2.imshow(\"test\", eye_left)\n",
    "        cv2.imshow(\"test\", frame)\n",
    "        frame_cnt+=1\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'): # 0또는 1의 경우 계속적으로 읽어옴\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webcam = cv2.VideoCapture('d:/drowsiness_video_data/SGA2100300S0042.mp4')\n",
    "\n",
    "# webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640) #1920, 640\n",
    "# webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) #1080, 480\n",
    "\n",
    "# if not webcam.isOpened():\n",
    "#     print(\"Could not open webcam\")\n",
    "#     exit()\n",
    "    \n",
    "\n",
    "# while webcam.isOpened():\n",
    "#     status, frame = webcam.read()\n",
    "\n",
    "#     if status:\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  #외부 영상 재생 시에 설정\n",
    "\n",
    "#         cv2.imshow(\"test\", frame)\n",
    "\n",
    "#     if cv2.waitKey(30) & 0xFF == ord('q'): # 0또는 1의 경우 계속적으로 읽어옴\n",
    "#         break\n",
    "\n",
    "# webcam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
