{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe & OpenCV project : Drowsiness Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe를 이용해 양쪽 눈에 대한 landmark(index) 포인트를 가져옴\n",
    "\n",
    "mp_facemesh = mp.solutions.face_mesh\n",
    "mp_drawing  = mp.solutions.drawing_utils\n",
    "denormalize_coordinates = mp_drawing._normalized_to_pixel_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceMash 설정\n",
    "\n",
    "def get_facemesh(\n",
    "                max_num_faces=1,                # 감지할 얼굴 수\n",
    "                refine_landmarks=False,         # 눈 외의 landmark는 세분화시키지 않음\n",
    "                min_detection_confidence=0.1,   # 얼굴 인식에 성공한 것으로 간주되는 최소 신뢰도\n",
    "                min_tracking_confidence= 0.2    # 성공적으로 추적한 것으로 간주되는 최소 신뢰도\n",
    "):\n",
    "    face_mesh = mp_facemesh.FaceMesh(\n",
    "        max_num_faces=max_num_faces,\n",
    "        refine_landmarks=refine_landmarks,\n",
    "        min_detection_confidence=min_detection_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence\n",
    "    )\n",
    "\n",
    "    return face_mesh\n",
    "# 감지된 landmark points 목록\n",
    "# face_mesh.multi_face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point_1, point_2):\n",
    "    # L2 norm 계산 (두 벡터 사이의 거리 계산)\n",
    "    dist = sum([(i - j) ** 2 for i, j in zip(point_1, point_2)]) ** 0.5\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR 공식 적용\n",
    "\n",
    "def get_ear(landmarks, refer_idxs, frame_width, frame_height):\n",
    "    # landmarks : 검출된 lanmarks list\n",
    "    # refer_idxs : 검출을 위해 지정한 landmarks list [index]\n",
    "\n",
    "    try:\n",
    "        # 수평 거리 계산\n",
    "        coords_points = []\n",
    "        for i in refer_idxs:\n",
    "            lm = landmarks[i]\n",
    "            coord = denormalize_coordinates(lm.x, lm.y, frame_width, frame_height)\n",
    "            coords_points.append(coord)\n",
    " \n",
    "        # EAR 공식에 맞춰 P2-P6, P3-P5, P1-P4를 연산함\n",
    "        P2_P6 = distance(coords_points[1], coords_points[5])\n",
    "        P3_P5 = distance(coords_points[2], coords_points[4])\n",
    "        P1_P4 = distance(coords_points[0], coords_points[3])\n",
    " \n",
    "        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)\n",
    " \n",
    "    except:\n",
    "        ear = 0.0\n",
    "        coords_points = None\n",
    " \n",
    "    return ear, coords_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_ear(landmarks, left_eye_idxs, right_eye_idxs, image_w, image_h):\n",
    "    \n",
    "    # 왼쪽 눈의 EAR 값과 landmarks의 좌표 값을 반환함\n",
    "    left_ear, left_lm_coordinates = get_ear(\n",
    "                                      landmarks, \n",
    "                                      left_eye_idxs, \n",
    "                                      image_w, \n",
    "                                      image_h\n",
    "                                    )\n",
    "    \n",
    "    # 오른쪽 눈의 EAR 값과 landmarks의 좌표 값을 반환함\n",
    "    right_ear, right_lm_coordinates = get_ear(\n",
    "                                      landmarks, \n",
    "                                      right_eye_idxs, \n",
    "                                      image_w, \n",
    "                                      image_h\n",
    "                                    )\n",
    "    # 최종 EAR 값을 얻기 위해 왼쪽 오른쪽의 EAR 값 평균을 계산함\n",
    "    Avg_EAR = (left_ear + right_ear) / 2.0\n",
    " \n",
    "    return Avg_EAR, (left_lm_coordinates, right_lm_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eye_landmarks(frame, left_lm_coordinates, \n",
    "                       right_lm_coordinates, color\n",
    "                       ):\n",
    "    for lm_coordinates in [left_lm_coordinates, right_lm_coordinates]:\n",
    "        if lm_coordinates:\n",
    "            for coord in lm_coordinates:\n",
    "                cv2.circle(frame, coord, 2, color, -1)\n",
    " \n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text(image, text, origin, \n",
    "              color, font=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "              fntScale=0.8, thickness=2\n",
    "              ):\n",
    "    image = cv2.putText(image, text, origin, font, fntScale, color, thickness)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eyes(crop_eye):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchsummary\n",
    "\n",
    "    from torchvision import models\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "\n",
    "    Class_Names = ['Closed_Eyes', 'Open_Eyes']\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = torch.load('drowsiness_detect.pt')\n",
    "    model.load_state_dict(torch.load('drowsiness_detect_state_dict.pt')) \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    eye_rgb = cv2.cvtColor(crop_eye, cv2.COLOR_BGR2RGB)\n",
    "    eye_img = Image.fromarray(eye_rgb)\n",
    "\n",
    "    eye_img = transform(eye_img).to(device)\n",
    "    eye_img = eye_img.unsqueeze(0)\n",
    "\n",
    "    test_preds = model(eye_img)\n",
    "    predicted_class_idx = torch.argmax(test_preds, dim=1)\n",
    "    predicted_class_label = Class_Names[predicted_class_idx.item()]\n",
    "\n",
    "    return predicted_class_label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(frame: np.array, thresholds : dict):\n",
    "\n",
    "        frame.flags.writeable = False\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "        \n",
    "        DROWSY_TIME_txt_pos = (10, int(frame_h // 2 * 1.7))\n",
    "        ALM_txt_pos = (10, int(frame_h // 2 * 1.85))\n",
    "\n",
    "        red = (0,0,255)\n",
    "        green = (0,255,0)\n",
    " \n",
    "        facemesh_model = get_facemesh()\n",
    "        results = facemesh_model.process(frame)\n",
    "\n",
    "        chosen_left_eye_idxs  = [362, 385, 387, 263, 373, 380]\n",
    "        chosen_right_eye_idxs = [33,  160, 158, 133, 153, 144]\n",
    " \n",
    "        state_tracker = {\n",
    "            \"start_time\": time.perf_counter(),\n",
    "            \"DROWSY_TIME\": 0.0,  # Holds time passed with EAR < EAR_THRESH\n",
    "            \"COLOR\": green,\n",
    "            \"play_alarm\": False,\n",
    "        }\n",
    "\n",
    "        if results.multi_face_landmarks:        \n",
    "\n",
    "            landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "            ############################################################### EAR 계산 및 감지한 eye landmark 시각화\n",
    "            EAR, coordinates = calculate_avg_ear(landmarks,\n",
    "                                                 chosen_left_eye_idxs, \n",
    "                                                 chosen_right_eye_idxs, \n",
    "                                                 frame_w, \n",
    "                                                 frame_h\n",
    "                                                 )\n",
    "            frame = plot_eye_landmarks(frame, \n",
    "                                       coordinates[0], \n",
    "                                       coordinates[1],\n",
    "                                       state_tracker[\"COLOR\"]\n",
    "                                       )\n",
    "            \n",
    "            ############################################################### model에 적용할 eye 영역 추출 및 결과 출력\n",
    "            global eye\n",
    "            eye = frame[coordinates[1][2][1]-10 : coordinates[0][4][1]+10, coordinates[1][0][0]-10 : coordinates[0][3][0]+10]\n",
    "\n",
    "            global model_result\n",
    "            model_result = pred_eyes(eye)\n",
    "\n",
    "\n",
    "            ############################################################### EAR 연산값 출력 및 알람 기능\n",
    "            if EAR < thresholds[\"EAR_THRESH\"]:\n",
    " \n",
    "                end_time = time.perf_counter()\n",
    "                # end_time = time.process_time()\n",
    " \n",
    "                state_tracker[\"DROWSY_TIME\"] += end_time - state_tracker[\"start_time\"]\n",
    "                state_tracker[\"start_time\"] = end_time\n",
    "                state_tracker[\"COLOR\"] = red\n",
    " \n",
    "                if state_tracker[\"DROWSY_TIME\"] >= thresholds[\"WAIT_TIME\"]:\n",
    "                    state_tracker[\"play_alarm\"] = True\n",
    "                    plot_text(frame, \"WAKE UP! WAKE UP\", \n",
    "                              ALM_txt_pos, state_tracker[\"COLOR\"])\n",
    " \n",
    "            else:\n",
    "                state_tracker[\"start_time\"] = time.perf_counter() #time.process_time()\n",
    "                state_tracker[\"DROWSY_TIME\"] = 0.0\n",
    "                state_tracker[\"COLOR\"] = green\n",
    "                state_tracker[\"play_alarm\"] = False\n",
    " \n",
    "            EAR_txt = f\"EAR: {round(EAR, 2)}\"\n",
    "            DROWSY_TIME_txt = f\"DROWSY: {round(state_tracker['DROWSY_TIME'], 3)} Secs\"\n",
    "            plot_text(frame, EAR_txt, \n",
    "                      (10,30), state_tracker[\"COLOR\"])\n",
    "            plot_text(frame, DROWSY_TIME_txt, \n",
    "                      DROWSY_TIME_txt_pos, state_tracker[\"COLOR\"])\n",
    " \n",
    "        else:\n",
    "            state_tracker[\"start_time\"] = time.perf_counter() #time.process_time()\n",
    "            state_tracker[\"DROWSY_TIME\"] = 0.0\n",
    "            state_tracker[\"COLOR\"] = green\n",
    "            state_tracker[\"play_alarm\"] = False\n",
    " \n",
    "            # frame = cv2.flip(frame, 1)\n",
    " \n",
    "        return frame, state_tracker[\"play_alarm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n",
      "Open_Eyes\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# webcam = cv2.VideoCapture(0)\n",
    "webcam = cv2.VideoCapture('d:/drowsiness_video_data/SGA2100300S0042.mp4')\n",
    "\n",
    "webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640) #1920, 640\n",
    "webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) #1080, 480\n",
    "\n",
    "thresholds = {\n",
    "    \"EAR_THRESH\": 0.25,\n",
    "    \"WAIT_TIME\": 0.1,\n",
    "}\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "    \n",
    "\n",
    "while webcam.isOpened():\n",
    "    status, frame = webcam.read()\n",
    "\n",
    "    if status:\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        frame, play_alarm = process(frame, thresholds)\n",
    "        print(model_result)\n",
    "\n",
    "        cv2.imshow(\"test\", frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'): # 0또는 1의 경우 계속적으로 읽어옴\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
